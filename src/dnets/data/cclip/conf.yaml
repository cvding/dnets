inference:
  common:
    device: 0
    #model_path: "/home/dingzhifeng/.cache/modelscope/hub/damo/multi-modal_clip-vit-base-patch14_zh/pytorch_model.bin"
    model_path: "/home/dingzhifeng/.cache/modelscope/hub/damo/multi-modal_clip-vit-base-patch16_zh/pytorch_model.bin"
    use: 'base'
    input_resolution: &input_resolution 224
    context_len: 32
  desc:
    url: https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh/summary

CLIP:
  large:
    embed_dim: 768
    image_resolution: *input_resolution
    vision_layers: 24
    vision_width: 1024
    vision_patch_size: 14

    vocab_size: 21128
    text_attention_probs_dropout_prob: 0.1 
    text_hidden_act: "gelu" 
    text_hidden_dropout_prob: 0.1 
    text_hidden_size: 768
    text_initializer_range: 0.02 
    text_intermediate_size: 3072 
    text_max_position_embeddings: 512 
    text_num_attention_heads: 12 
    text_num_hidden_layers: 12
    text_type_vocab_size: 2
  base:
    embed_dim: 512
    image_resolution: *input_resolution
    vision_layers: 12
    vision_width: 768
    vision_patch_size: 16

    vocab_size: 21128
    text_attention_probs_dropout_prob: 0.1 
    text_hidden_act: "gelu" 
    text_hidden_dropout_prob: 0.1 
    text_hidden_size: 768
    text_initializer_range: 0.02 
    text_intermediate_size: 3072 
    text_max_position_embeddings: 512 
    text_num_attention_heads: 12 
    text_num_hidden_layers: 12 
    text_type_vocab_size: 2 